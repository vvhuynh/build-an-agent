{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbed95a-4ffe-4528-8e6d-08e2bcd9149f",
   "metadata": {},
   "source": [
    "# Research Agent Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed12a14e-9121-4067-aa02-861b6c7b7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt > /dev/null\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(\"../variables.env\")\n",
    "_ = load_dotenv(\"../secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907fa086-42ee-40a0-8e60-d392de19b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from docgen_agent.researcher import ResearcherState, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b4043-81d6-4a1b-b432-b07fb3d043a0",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "Try making changes to the agent request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c1ee85-8472-46be-bbe9-66a14e93c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Examples of AI agents in various industries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ce016-b2f9-42bc-841b-5670b20a8688",
   "metadata": {},
   "source": [
    "## Ask the Agent\n",
    "\n",
    "Here, we will send your request to the agent. The agent will print log messages to let you know what it is up to. Expect this step to take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0669ee3-902c-44a0-8916-147e53a4fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docgen_agent.researcher:Calling model.\n",
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[401] Unauthorized\nAuthentication failed\nPlease check or regenerate your API key.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m state = ResearcherState(\n\u001b[32m      2\u001b[39m     topic=topic,\n\u001b[32m      3\u001b[39m     number_of_queries=\u001b[32m3\u001b[39m,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m state = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2920\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2917\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2918\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2920\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2921\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2922\u001b[39m     config,\n\u001b[32m   2923\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   2926\u001b[39m     print_mode=print_mode,\n\u001b[32m   2927\u001b[39m     output_keys=output_keys,\n\u001b[32m   2928\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2929\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2930\u001b[39m     **kwargs,\n\u001b[32m   2931\u001b[39m ):\n\u001b[32m   2932\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2933\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2768\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2767\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2768\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2769\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2770\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2771\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2772\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2773\u001b[39m ):\n\u001b[32m   2774\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2775\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2776\u001b[39m         stream_mode,\n\u001b[32m   2777\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2780\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2781\u001b[39m     ):\n\u001b[32m   2782\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/code/docgen_agent/researcher.py:59\u001b[39m, in \u001b[36mcall_model\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_MAX_LLM_RETRIES):\n\u001b[32m     58\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt}] + \u001b[38;5;28mlist\u001b[39m(state.messages)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools.ainvoke(messages, config)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/runnables/base.py:5447\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5442\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5445\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5446\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5448\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5449\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5450\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:400\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    392\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     **kwargs: Any,\n\u001b[32m    398\u001b[39m ) -> BaseMessage:\n\u001b[32m    399\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    401\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    402\u001b[39m         stop=stop,\n\u001b[32m    403\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    408\u001b[39m         **kwargs,\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:974\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    967\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m     **kwargs: Any,\n\u001b[32m    972\u001b[39m ) -> LLMResult:\n\u001b[32m    973\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    975\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:932\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    920\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    921\u001b[39m             *[\n\u001b[32m    922\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m             ]\n\u001b[32m    931\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    933\u001b[39m flattened_outputs = [\n\u001b[32m    934\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    936\u001b[39m ]\n\u001b[32m    937\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1100\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1098\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1101\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1140\u001b[39m, in \u001b[36mBaseChatModel._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1134\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1137\u001b[39m     **kwargs: Any,\n\u001b[32m   1138\u001b[39m ) -> ChatResult:\n\u001b[32m   1139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Top Level call.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1142\u001b[39m         \u001b[38;5;28mself\u001b[39m._generate,\n\u001b[32m   1143\u001b[39m         messages,\n\u001b[32m   1144\u001b[39m         stop,\n\u001b[32m   1145\u001b[39m         run_manager.get_sync() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1146\u001b[39m         **kwargs,\n\u001b[32m   1147\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/runnables/config.py:616\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    615\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    617\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    618\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    619\u001b[39m     )\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/runnables/config.py:607\u001b[39m, in \u001b[36mrun_in_executor.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m() -> T:\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    609\u001b[39m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[32m    610\u001b[39m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[32m    611\u001b[39m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:410\u001b[39m, in \u001b[36mChatNVIDIA._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m inputs, extra_headers = _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m._client.model)\n\u001b[32m    409\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_payload(inputs=inputs, stop=stop, stream=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m responses, _ = \u001b[38;5;28mself\u001b[39m._client.postprocess(response)\n\u001b[32m    412\u001b[39m \u001b[38;5;28mself\u001b[39m._set_callback_out(responses, run_manager)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:482\u001b[39m, in \u001b[36m_NVIDIAClient.get_req\u001b[39m\u001b[34m(self, payload, extra_headers)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_req\u001b[39m(\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    478\u001b[39m     payload: \u001b[38;5;28mdict\u001b[39m = {},\n\u001b[32m    479\u001b[39m     extra_headers: \u001b[38;5;28mdict\u001b[39m = {},\n\u001b[32m    480\u001b[39m ) -> Response:\n\u001b[32m    481\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Post to the API.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     response, session = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait(response, session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:378\u001b[39m, in \u001b[36m_NVIDIAClient._post\u001b[39m\u001b[34m(self, invoke_url, payload, extra_headers)\u001b[39m\n\u001b[32m    374\u001b[39m session = \u001b[38;5;28mself\u001b[39m.get_session_fn()\n\u001b[32m    375\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response = session.post(\n\u001b[32m    376\u001b[39m     **\u001b[38;5;28mself\u001b[39m.__add_authorization(\u001b[38;5;28mself\u001b[39m.last_inputs)\n\u001b[32m    377\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:471\u001b[39m, in \u001b[36m_NVIDIAClient._try_raise\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    469\u001b[39m     body += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease check or regenerate your API key.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: [401] Unauthorized\nAuthentication failed\nPlease check or regenerate your API key.",
      "During task with name 'agent' and id '4c46c151-6f87-7257-ca3a-8c38ce97e7e4'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "state = ResearcherState(\n",
    "    topic=topic,\n",
    "    number_of_queries=3,\n",
    ")\n",
    "\n",
    "state = await graph.ainvoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedc2f-164c-4ace-8f5b-cf27eefadbd2",
   "metadata": {},
   "source": [
    "## Examine the results\n",
    "\n",
    "The researcher adds the research results to the chat log so that it is available to all later steps.\n",
    "\n",
    "Let's examine our conversation with the researcher.\n",
    "\n",
    "You should see at least three messages:\n",
    "1. A message from the model requesting a tool call.\n",
    "2. A tool_call message with the raw source data.\n",
    "3. A message from the assistant, summarizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe890305-0f1c-4c8d-a918-b5d0dbbfbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in state[\"messages\"]:\n",
    "    print(\"ROLE: \", getattr(message, \"role\", \"tool_call\"))\n",
    "    print(message.content[:500] or message.additional_kwargs)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2908c3-f9f8-4cd3-b68d-fc3a56ca645c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
