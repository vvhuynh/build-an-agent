{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da39e56-6555-48e0-bb4e-49a438918763",
   "metadata": {},
   "source": [
    "\n",
    "  # Agents the Hard Way - Complete Solution\n",
    "\n",
    "In this exercise, we will build a simple AI agent from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891cc8f-25d6-4ac0-9ccc-e16aa1383f66",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "Like most Python code, we start by importing the necessary modules.\n",
    "- The `openai` module is maintained by OpenAI and is used for talking to models running remotely.\n",
    "- The `caching` module helps us cache LLM responses and conserve tokens.\n",
    "\n",
    "The caching module is custom code, but it makes use of the `cachier` Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bac2ae88-9b54-486f-a340-609d946ea46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_llm_cached(model_client, model_name, message_history, tool_list):\n",
    "    \"\"\"Simple wrapper for OpenAI API calls without caching.\"\"\"\n",
    "    kwargs = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": message_history,\n",
    "    }\n",
    "    \n",
    "    if tool_list:\n",
    "        kwargs[\"tools\"] = tool_list\n",
    "    \n",
    "    response = model_client.chat.completions.create(**kwargs)\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    result = {\"role\": \"assistant\", \"content\": message.content}\n",
    "    \n",
    "    if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "        result[\"tool_calls\"] = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            result[\"tool_calls\"].append({\n",
    "                \"id\": tool_call.id,\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"arguments\": tool_call.function.arguments\n",
    "                },\n",
    "                \"type\": \"function\"\n",
    "            })\n",
    "        result[\"content\"] = None\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f5940-34a9-4ee7-9bb8-5a853b12e5a9",
   "metadata": {},
   "source": [
    "## Load the configuration\n",
    "\n",
    "Now I'll load our configuration as constants:\n",
    "- `API_KEY` loads our credentials from an environment variable\n",
    "- `MODEL_URL` points to the server hosting your model\n",
    "- `MODEL_NAME` is the model we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c87505ea-dc6d-4d0e-83fb-44512522ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \" \"  # Replace with your real key within \"\"\n",
    "MODEL_URL = \"https://integrate.api.nvidia.com/v1\"\n",
    "MODEL_NAME = \"meta/llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728524a-03b3-45f6-87d4-2c5d5494823d",
   "metadata": {},
   "source": [
    "## Part 1 - The Model\n",
    "The first of four critical parts for an agent is the AI model.\n",
    "\n",
    "We will talk to the AI models on build.nvidia.com using the OpenAI API.\n",
    "This API is the *language* that most model providers use.\n",
    "This means we can use the `OpenAI` class to connect to most model providers. Neat!\n",
    "\n",
    "Using the `MODEL_URL` and `API_KEY` defined above, create a new model client named `client`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1133e2-be04-4fb2-88c6-2693e8ada007",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "client = OpenAI(base_url=MODEL_URL, api_key=API_KEY)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17f35714-333b-4f49-9dd7-7385c732bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0edb59-2751-4d9d-a9df-ad2f40276ec9",
   "metadata": {},
   "source": [
    "## Why these values?\n",
    "\n",
    "This `MODEL_URL` points to NVIDIA's hosted Model API Catalog.\n",
    "\n",
    "Because we are starting with NVIDIA's hosted service, we have a lot of models to choose from. Picking where to start can be difficult.\n",
    "\n",
    "Start with a newer open source model from a team you recognize.\n",
    "Start with a moderate sized model (~70b parameters). You'll work on optimizing to a smaller model later.\n",
    "If you need features like function calling, make sure the model supports it! (More on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226cf9f-dac1-483e-84a1-57f0648a689e",
   "metadata": {},
   "source": [
    "## Part 2 - Tools\n",
    "\n",
    "Every agent has access to some tools.\n",
    "Tools are how the model is able to interact with the world.\n",
    "\n",
    "Tools are simply code that is executed at the LLM'srequest.\n",
    "So let's write our first tool!\n",
    "\n",
    "Create a function, called `add`, that adds two integers, called `a` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8cb06-95ae-4dfa-8b66-24c5884e6a98",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8f9469f-e183-48d1-9c57-d4b193976a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your add function here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8b855-f766-4b60-a2ad-cc3f5a808fd0",
   "metadata": {},
   "source": [
    "## Describe the tools\n",
    "\n",
    "Before moving on, we need to create a description of the tools that the model can understand.\n",
    "Think of this as the LLM's menu of possible helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1508be56-8d97-401f-ac17-c75609b58310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"add\",\n",
    "            \"description\": \"Add two integers.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"a\": {\"type\": \"integer\", \"description\": \"First integer\"},\n",
    "                    \"b\": {\"type\": \"integer\", \"description\": \"Second integer\"},\n",
    "                },\n",
    "                \"required\": [\"a\", \"b\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28e26a-7d20-4f32-81db-703c5f6b3e1d",
   "metadata": {},
   "source": [
    "## Part 3 - Memory\n",
    "\n",
    "The topic of memory is complex, and we will only scratch the surface.\n",
    "There are two types of memory, short term and long term.\n",
    "For now, let's focus on short term memory.\n",
    "\n",
    "Short term memory starts at the beginning of the conversation and ends at the end of the conversation.\n",
    "Put simply, short term memory is a log of the conversation.\n",
    "\n",
    "For this, we will use a humble list.\n",
    "Every line in our list will be a message in the conversation, stored in a dictionary.\n",
    "The messages can come from the user, the assistant, or from tools.\n",
    "\n",
    "Create an initial list called `memory`.\n",
    "Initialize it with this message from the user: \"What is 3 plus 12?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058b92b-2e00-498b-87f2-69077a92d63e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "memory = [\n",
    "  {\"role\": \"user\", \"content\": \"What is 3 plus 12?\"}\n",
    "]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90db9590-d436-42ae-8e69-5afee4a0f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5691a-ab9d-44e4-ac39-5d63263843af",
   "metadata": {},
   "source": [
    "## Run the agent\n",
    "\n",
    "We now have three of the four pieces required of an agent. The last missing piece is the routing. For the time being, we will forgo this piece and manually route this message.\n",
    "\n",
    "The agent starts by giving the memory and tool list to the model. The model will reply by either requesting a tool or answering the question.\n",
    "\n",
    "Based on the model's response, we will decide how to proceed.\n",
    "\n",
    "Call the model using the `call_llm_cached` function. The function takes four arguments:\n",
    "\n",
    "- `model_client`: the OpenAI client\n",
    "- `model_name`: the name of the model to use (check the constants from before)\n",
    "- `message_history`: your short term memory\n",
    "- `tool_list`: the menu of tools the model can access\n",
    "\n",
    "Use this function to call the LLM. Save the result by appending it to the end of `messages`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb34322-ff28-4e4c-a122-c6381b0d03b5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "llm_response = call_llm_cached(client, MODEL_NAME, memory, tools)\n",
    "memory.append(llm_response)\n",
    "\n",
    "print(llm_response) \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "465b0b21-e3ae-462e-8386-67bf80b9402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-f485374cbf09466a9ddef47ede3cabb0', 'function': {'name': 'add', 'arguments': '{\"a\": 3, \"b\": 12}'}, 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6039e-04b7-490d-ad92-054cd2e78d65",
   "metadata": {},
   "source": [
    "## Part 4 - Routing\n",
    "\n",
    "Looks like the model chose to request a tool instead of answering. We can tell because `content` is `null` and a function is defined under `tool_calls`.\n",
    "\n",
    "> **Tools vs Function Calling?**  \n",
    "> These terms are often used interchangeably. Technically, Function Calling is a feature of a model. This feature allows the model to request that the developer run the Tools.  \n",
    "> But most of the time, these terms are simply referring to an agent's ability to run functions.\n",
    "\n",
    "We can see that the model has requested that we run the `add` function with the arguments 3 and 12.\n",
    "\n",
    "Write some code to extract the requested function's name, arguments, and id from `messages[-1]`.  \n",
    "Store those values in variables called `tool_name`, `tool_args`, and `tool_id` respectively.\n",
    "\n",
    "ðŸ’¡ **TIP:** The value of `arguments` is a string. Use the `json` library to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f306b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1820c2e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "tool_call = memory[-1][\"tool_calls\"][0]\n",
    "tool_name = tool_call[\"function\"][\"name\"]\n",
    "tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "tool_id = tool_call[\"id\"]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def6fc0-60aa-4edf-af62-eb89e30e45d8",
   "metadata": {},
   "source": [
    "If you haven't noticed by now, even though the feature is called tool calling...  \n",
    "The model doesn't actually call the tool!\n",
    "\n",
    "So let's write the code to run the tools as requested.\n",
    "\n",
    "Check if the tool name is equal to `add`. If it is, then run the add function with the requested arguments.\n",
    "\n",
    "Save the output from the tool call to a variable called `tool_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "019b5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e3ef9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "if tool_name == \"add\":\n",
    "    tool_out = add(**tool_args)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5438710-0b1f-42c1-9cc5-dfa94613a8ff",
   "metadata": {},
   "source": [
    "We just got `tool_out` back from the tool. Now we can update the memory with the tool output.\n",
    "\n",
    "Next time we call the model, it will see the prompt from the user, its own request for a tool call, and the result of that tool call.\n",
    "\n",
    "This is another one where the syntax is tricky and picky. This is the standard message format for a tool call result (as defined by OpenAI):\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"What is 3 plus 12?\"},\n",
    "  {\"role\": \"assistant\", \"tool_calls\": ... },\n",
    "  {\"role\": \"tool\", \"tool_call_id\": \"...\", \"name\": \"add\", \"content\": \"15\"}\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e69fe0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112866af",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ NEED SOME HELP?</summary>\n",
    "\n",
    "```python\n",
    "tool_result = {\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_id,\n",
    "    \"name\": tool_name,\n",
    "    \"content\": str(tool_out)\n",
    "}\n",
    "memory.append(tool_result)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0afe332-c9d2-4e3a-a558-d06daf7696b9",
   "metadata": {},
   "source": [
    "Now, we call the model again and save the response in memory.\n",
    "\n",
    "ðŸ’¡ **HINT:** It is the exact same code as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba523c1a-8849-4b38-bbb8-d5c564b58f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what the model had to say:\n",
      "\n",
      "{'role': 'assistant', 'content': 'The answer is 15.'}\n"
     ]
    }
   ],
   "source": [
    "llm_response = call_llm_cached(client, MODEL_NAME, memory, tools)\n",
    "memory.append(llm_response)\n",
    "print(\"Here is what the model had to say:\\n\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e460b-932e-471f-b545-748a194acfb2",
   "metadata": {},
   "source": [
    "## Complete Conversation History\n",
    "\n",
    "Let's see the full memory to understand how the agent processed the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ef69e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'What is 3 plus 12?', 'role': 'user'},\n",
      " {'content': None,\n",
      "  'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'add'},\n",
      "                  'id': 'chatcmpl-tool-f485374cbf09466a9ddef47ede3cabb0',\n",
      "                  'type': 'function'}]},\n",
      " {'content': '15',\n",
      "  'name': 'add',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'chatcmpl-tool-f485374cbf09466a9ddef47ede3cabb0'},\n",
      " {'content': 'The answer is 15.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a781b-2067-45b4-a9c5-35d51d38161b",
   "metadata": {},
   "source": [
    " # You've Reached The End! \n",
    "\n",
    " Congrats on making your very first agent. But you might be wondering, now what? Keep going through this workshop to find out. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
